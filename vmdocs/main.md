# CPython 3.11 Interpreter Docs

## RECONSIDER -- what VM state to describe first?

Borrow from the great CPU architecture manuals.

A possible logical order:

- The role of the compiler
- Code object
- Frame object
- Instruction stream
  - first_instr and next_instr
  - opcode and oparg
  - EXTENDED_ARG
  - absolute and relative jumps
  - inline caches (forward ref to quickening/specializing)
- Locals "plus"
  - Stack and stack_pointer
  - Locals
  - Arguments
  - Cells
- Globals and builtins
- Exception handling
- Generators and async functions
  - yield
  - yield from
  - return from generator
  - await
  - async generators
- Line and column numbers
- Tracing
- The eval breaker
- Inline caches
- Quickening and specialization
  - Lots of subsections?

## Introduction

The CPython interpreter (a.k.a. VM -- Virtual Machine; unrelated to the OS-level concept of that name) changed dramatically in CPython 3.11.
This writeup attempts to document the 3.11 VM.
Note that 3.12 will evolve the VM again; once the 3.11 version is reasonably complete I will attempt an update for 3.12.

## Preliminary reading

Earlier I started writing [Python Interpreter State](https://gvanrossum.github.io/formal/state.html), a more abstract writeup on the same subject.
That writeup describes a hypotetical Python VM that is supposed to have the same semantics as CPython, except for memory use, finalization order and a host of other esoteric details (line numbers, tracing, and more).
Here I actually try to go into the nitty-gritty detail of CPython 3.11.

Other useful references:
- Anthony Shaw's "CPython Internals". The first edition describes CPython 3.9.
- The devguide's section on [Compiler Design](https://devguide.python.org/internals/compiler/).
- The docs for [`dis.py`](https://docs.python.org/3/library/dis.html#python-bytecode-instructions) in 3.11.
- [PEP 659, Specializing Adaptive Interpreter](https://peps.python.org/pep-0659/).
- [`Python/adaptive.md`](https://github.com/python/cpython/blob/main/Python/adaptive.md) in the source tree.

## The role of the compiler

The CPython VM is designed for code generated by the CPython bytecode compiler.
Apart from the usual tasks like parsing, the compiler is depended on for scope analysis (separating name references into locals, nonlocals and globals), to assign indexes to local variables, and (maybe most importantly) to calculate the stack size needed for each function.

## The code object

All the output of the compiler is combined in the code object.
The interpreter always requires a code object (passed to it in the initial frame) and it uses the code object to find many disparate data items, from the bytecode and the stack size to the mapping from bytecode to line numbers and the names of the arguments.

(An exception are the default argument values for functions -- these are computed after the code object has already been constructed, and are stored on the function object.
The defaults are merged with the arguments specified in a call and placed on the caller's stack before the callee's stack frame is created.)

Code objects are immutable and hashable (usable as dict keys).
In 3.11 (and to some extent in earlier versions) not all fields of a code object are actually constant -- in particular, the bytecode is dynamically updated by the specializer.
This is almost invisible at the Python level -- the `co_code` attribute (which in Python 3.10 and before returned the very bytes object containing the bytecode generated by the compiler) is a property that dynamically reconstructs the unspecialized bytecode.
(Its result is cached to avoid the overhead of this reconstruction on further references.)

For experts, the `co_code_adaptive` attribute returns a copy of the bytecode as currently seen by the interpreter.
This attribute (like a few others that are not always constant) is not considered by the `__hash__` and `__eq__` special methods on code objects.

Finally, note that the `co_code_adaptive` field in C is actually a dynamically sized array tacked onto the end of the code object.
This saves a memory allocation when creating a code object and increases memory locality (the start of the bytecode is close to the rest of the code object in memory).

## The frame object

XXX

## VM state

The basic VM design uses a stack machine for evaluating instructions.
The stack is "segmented", and only used for expression evaluation -- the call stack is represented differently.
Instructions are fairly high level, e.g. "load a variable", "add two objects", "call a function" and so on.
Complications exist due to Python semantics, e.g. exception handling, and performance considerations, e.g. optimized attribute access.



### Mutable VM state

Here's an incomplete list of the various bits of mutable state.

- A pointer to the next instruction.
  This is manipulated by instruction decoding and by jumps.

- The expression evaluation stack: an array of object pointers and a pointer to the top of the stack (the stack grows up).
  The stack size is precomputed by the bytecode compiler and stored in the code object; except in debug mode there is no stack overflow or underflow checking.

- An array of optimized ("fast") local variables.
  This array is contiguous with the stack, and they are allocated together.
  In class and global scopes this array is empty.
  In function scopes the first `argcount` variables are initialized from the call arguments (possibly from the default values in the function object).
  Local variables may be "cell" variables.

  A cell is a tiny object that wraps a pointer to another object.
  Cells are used for "free variables" (nonlocals, a.k.a. closures), initialized from a list of cells in the function object.
  Local variables that are referenced by nested functions are also represented as cells, and are initialized using the `MAKE_CELL` opcode.
  The compiler determines which local variables contain cells and which contain regular objects, and generates different code accordingly.

  There is a case where a cell begins its life as a regular object: when a function argument is referenced by a nested function.
  In this case `MAKE_CELL` wraps the regular object in a cell; otherwise the cell is initialized with a `NULL` pointer.

- Flags related to interrupting a thread between instructions (e.g. to invoke a signal handler, or to drop the GIL).

- Flags related to tracing, a mechanism typically used by debuggers and profilers to arrange for callbacks when certain events happen during execution (e.g. a function call or return, or execution moves to a different line in the source code).

### Immutable VM state

There is also VM state that is initialized once when execution of a particular code object begins.

The following pointers point to data that is itself mutable -- it's just the pointer that's immutable.

- A pointer to the array of bytecode instructions.
  In 3.11 and later the bytecode is a mutable array!
  At times instructions are replaced with other by a process called "specialization".
  Additionally, some instructions are followed by what's aptly called an "in-line cache".
  The existence, size and use of in-line cache entries depends on the opcode.

- A pointer to the dict containing the global variables (usually initialized from a function object's `__globals__` attribute).

- A pointer to the dict containing the builtins (usually initialized from a special global named `__builtins__`).

- An optional pointer to a dict holding the local variables for a class or global scope.

The following pointers point to data that is truly immutable -- typically contained in the code object.

- A tuple of constants referenced by `LOAD_CONST`.

- A tuple of strings giving variable and attribute names used by instructions like `LOAD_NAME` and `LOAD_ATTR`.

- A pointer to a table used for exception handling, explained below.

- A pointer to a table used for reporting code locations (start and end line and column), also explained below.

## Instruction format and decoding

In Python 3.11 instructions have the following format:

- Zero or more two-byte `EXTENDED_ARG` pseudo-instructions used to extend the range of the oparg.
- A two-byte code unit giving the opcode and the (unextended) oparg, one byte each.
- Depending on the opcode, zero or more two-byte cache entries.

Thus, instructions are always two-byte aligned.
On big-endian machines, the opcode is the high byte and the oparg is the low byte; on little-endian machines (a dying race?) it's the other way around.

The `EXTENDED_ARG` mechanism is needed to allow instructions to have arguments larger than 256.
This is rare, but not impossible -- some large functions may have more than 256 locals, and jumps often have to jump over more than 256 code-units.

A single `EXTENDED_ARG` instruction prefixing a regular instruction provides bits 8-15 of the oparg.
Two `EXTENDED_ARG` instructions provide bits 16-23 and 8-15, respectively.
For example:
```
EXTENDED_ARG 1
EXTENDED_ARG 2
LOAD_FAST    3
```
effectively encodes a `LOAD_FAST` instruction with an oparg value of `66051` (i.e., `(1<<16) | (2<<8) | 3`).
Thus, we can think of this as a big-endian representation.
There is no way to represent negative oparg values.

Here's a simplified version of the instruction decoding and execution loop that takes `EXTENDED_ARG` into account:
```cc
_Py_CODEUNIT *next_instr = first_instr;
while (1) {
    oparg = _Py_OPARG(*next_instr);
  dispatch:
    opcode = _Py_OPCODE(*next_instr);
    next_instr++;
    switch (opcode) {
        case EXTENDED_ARG:
            oparg <<= 8;
            oparg |= _Py_OPARG(*next_instr);
            goto dispatch;
        // One case per instruction
    }
}
```
Note that at the top of the switch `next_instr` points to the instruction _after_ the one currently being executed.

Adjusting `next_instr` for in-line cache entries is simple: at the end of each instruction case that has a cache entry, bump `next_instr` by the number of (two-byte) cache entries.



## Code tour

The VM executes [code objects](XXXlink).
Execution starts with `_PyEval_EvalFrameDefault(tstate, frame, throwflag)`.
Here `tstate` points to the thread state (through which we can also reach the interpreter state and the runtime state), `frame` points to a frame that was freshly prepared just for this function call (here we find the code object to execute, the globals, and call arguments), and `throwflag` has to do with re-entering a generator using `g.throw()`.

Some VM state is kept in the frame object (e.g., the local variables and evaluation stack).
Other VM state is kept in local variables of `_PyEval_EvalFrameDefault` (e.g., instrunction decoding state for `EXTENDED_ARG`).
There is also "state" that is merely a cache for information that can be reconstructed from other state -- e.g., `consts`, which is `frame->f_code->co_consts`.

### The eval breaker

The first non-trivial data structure we encounter upon entry to `_PyEval_EvalFrameDefault` is the "eval breaker".
This is a per-interpreter flag that must be checked regularly in order to handle signals and other "interrupts".
This flag can be set by a signal handler or by another thread that doesn't have the GIL, so it must be atomic.

Since the actual atomic `eval_breaker` variable lives in the interpreter state, which requires a bit of pointer chasing to reach from `tstate` (the thread state pointer), the _address_ of the atomic variable is cached in a local variable, `eval_breaker` (a pointer to an atomic int).
At various points the eval breaker is checked, using the `CHECK_EVAL_BREAKER` macro.
When the flag is set, the macro jumps to the `handle_eval_breaker` label.
(Obviously this implies that the macro is only valid within the body of `_PyEval_EvalFrameDefault`.)

The `CHECK_EVAL_BREAKER` macro calls `_Py_atomic_load_relaxed_int32(eval_breaker)`, whose semantics are beyond the scope of this writeup -- basically it loads the value to which the pointer points, using compiler directives that assure the compiler won't optimize this load instruction away.
To set the eval breaker, code uses the `COMPUTE_EVAL_BREAKER` macro, which ORs together several other atomic values: `gil_drop_request`, `signals_pending`, and `pending.calls_to_do`, storing the result in `eval_breaker`.
These flags are set by various corresponding macros: `[RE]SET_GIL_DROP_REQUEST`, `[UN]SIGNAL_PENDING_CALLS`, `[UN]SIGNAL_PENDING_SIGNALS`, and `[UN]SIGNAL_ASYNC_EXC`.
The `COMPUTE_EVAL_BREAKER` macro takes three arguments: `interp`, `ceval`, and `ceval2`.
The latter two are passed as optimizations -- they correspond to `interp->runtime->ceval` and `interp->ceval`.

All those flags live in the interpreter state in the `interp->ceval` struct, except for `signals_pending`, which lives in the runtime state (since signal handlers are process-global).
There is also logic to only set `eval_breaker` (which is a "roll-up" flag for the other three) when the current thread is actually able (or allowed to) handle signals -- Python forces all Python-level signal handlers to run in the main thread.
Further details about signal handling, pending calls, and the GIL is out of scope here.

There is debate possible about when to call `CHECK_EVAL_BREAKER`.
We want it to be called frequently enough that the response is near instantaneous, but since the check is expensive (due to the atomic load), we don't want to call it too often.
The best compromise seems to be to call it on every back jump (so we can break out of infinite loops) and on every call (to break out of infinite recursion).

## The tracing flag

XXX

## Throwing an exception into a generator frame

XXX (Depends on error handling.)

## Python-to-Python calls

XXX

## Cached variables

XXX 